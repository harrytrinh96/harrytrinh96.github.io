{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TrinhDinhPhuc/CompoundClassification/blob/main/Compound_Classification_using_Graph_Convolutional_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6aJ-T7fR-pu",
    "outputId": "b160c68f-1563-45dc-d0b5-706cae96ca4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3490  100  3490    0     0  22229      0 --:--:-- --:--:-- --:--:-- 22229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "add /root/miniconda/lib/python3.6/site-packages to PYTHONPATH\n",
      "python version: 3.6.9\n",
      "fetching installer from https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
      "done\n",
      "installing miniconda to /root/miniconda\n",
      "done\n",
      "installing rdkit, openmm, pdbfixer\n",
      "added conda-forge to channels\n",
      "added omnia to channels\n",
      "done\n",
      "conda packages installation finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                  *  /root/miniconda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl -Lo conda_installer.py https://raw.githubusercontent.com/deepchem/deepchem/master/scripts/colab_install.py\n",
    "import conda_installer\n",
    "conda_installer.install()\n",
    "!/root/miniconda/bin/conda info -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4uE4lCuSEyS",
    "outputId": "e6b96e30-9d36-4154-da5c-20c83a389895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepchem\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/6e/38d3f46c7e33137f42bf6edae8d1d46734df29c699d752e72f1b3aa8b7e5/deepchem-2.5.0.dev20210219192147-py3-none-any.whl (548kB)\n",
      "\r",
      "\u001b[K     |▋                               | 10kB 22.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 20kB 17.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 30kB 14.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 40kB 13.0MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 51kB 8.9MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 61kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 71kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 81kB 10.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 92kB 9.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 102kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 112kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 122kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 133kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 143kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 153kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 163kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 174kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 184kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 194kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 204kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 215kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 225kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 235kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 245kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 256kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 266kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 276kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 286kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 296kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 307kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 317kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 327kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 337kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 348kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 358kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 368kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 378kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 389kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 399kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 409kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 419kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 430kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 440kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 450kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 460kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 471kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 481kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 491kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 501kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 512kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 522kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 532kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 542kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 552kB 8.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from deepchem) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.4.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from deepchem) (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->deepchem) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->deepchem) (1.15.0)\n",
      "Installing collected packages: deepchem\n",
      "Successfully installed deepchem-2.5.0.dev20210219192147\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre deepchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jeMlhjd5SIq3"
   },
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rdkit.Chem as Chem\n",
    "import rdkit.Chem.AllChem as AllChem\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfhrQj0QSMZL",
    "outputId": "10ee33da-dfc5-4b6f-8008-3313bccd4161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-20 10:22:42--  https://raw.githubusercontent.com/TrinhDinhPhuc/CompoundClassification/main/cmpd.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 530832 (518K) [text/plain]\n",
      "Saving to: ‘cmpd.csv’\n",
      "\n",
      "cmpd.csv            100%[===================>] 518.39K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-02-20 10:22:42 (16.4 MB/s) - ‘cmpd.csv’ saved [530832/530832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/TrinhDinhPhuc/CompoundClassification/main/cmpd.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "iDpX96drS1TO",
    "outputId": "5a46ea42-3fa6-4c40-b345-3b61843e9f24"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inchikey</th>\n",
       "      <th>smiles</th>\n",
       "      <th>group</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FNHKPVJBJVTLMP-UHFFFAOYSA-N</td>\n",
       "      <td>CNC(=O)c1cc(Oc2ccc(NC(=O)Nc3ccc(Cl)c(C(F)(F)F)...</td>\n",
       "      <td>train</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUDVHEFYRIWYQD-UHFFFAOYSA-N</td>\n",
       "      <td>CNC(=O)c1cccc2cc(Oc3ccnc4cc(OCC5(N)CC5)c(OC)cc...</td>\n",
       "      <td>train</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTZSNFLLYPYKIL-UHFFFAOYSA-N</td>\n",
       "      <td>Cc1cc2cc(Oc3ccnc(Nc4cccc(CS(=O)(=O)NCCN(C)C)c4...</td>\n",
       "      <td>test</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UOVCGJXDGOGOCZ-UHFFFAOYSA-N</td>\n",
       "      <td>COc1cc2c(cc1F)C(c1ccccc1Cl)=Nc1c(n[nH]c1C)N2</td>\n",
       "      <td>train</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUIHSIWYWATEQL-UHFFFAOYSA-N</td>\n",
       "      <td>Cc1ccc(Nc2nccc(N(C)c3ccc4c(C)n(C)nc4c3)n2)cc1S...</td>\n",
       "      <td>test</td>\n",
       "      <td>active</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      inchikey  ... activity\n",
       "0  FNHKPVJBJVTLMP-UHFFFAOYSA-N  ...   active\n",
       "1  CUDVHEFYRIWYQD-UHFFFAOYSA-N  ...   active\n",
       "2  TTZSNFLLYPYKIL-UHFFFAOYSA-N  ...   active\n",
       "3  UOVCGJXDGOGOCZ-UHFFFAOYSA-N  ...   active\n",
       "4  CUIHSIWYWATEQL-UHFFFAOYSA-N  ...   active\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmpd_df = pd.read_csv('cmpd.csv')\n",
    "cmpd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ejyg-F1AS3eg"
   },
   "outputs": [],
   "source": [
    "## Create a label column where activity.active = 1 otherwise = 0  \n",
    "cmpd_df[\"label\"] = None\n",
    "for row in cmpd_df.itertuples():\n",
    "  if row.activity == \"active\":\n",
    "    cmpd_df.at[row.Index,'label'] = 1\n",
    "  else:\n",
    "    cmpd_df.at[row.Index,'label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "dE2qP4MkS9ee",
    "outputId": "dab7f277-7eed-468e-8e33-9b681e451f5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inchikey</th>\n",
       "      <th>smiles</th>\n",
       "      <th>group</th>\n",
       "      <th>activity</th>\n",
       "      <th>label</th>\n",
       "      <th>smileToMol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FNHKPVJBJVTLMP-UHFFFAOYSA-N</td>\n",
       "      <td>CNC(=O)c1cc(Oc2ccc(NC(=O)Nc3ccc(Cl)c(C(F)(F)F)...</td>\n",
       "      <td>train</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;deepchem.feat.mol_graphs.ConvMol object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CUDVHEFYRIWYQD-UHFFFAOYSA-N</td>\n",
       "      <td>CNC(=O)c1cccc2cc(Oc3ccnc4cc(OCC5(N)CC5)c(OC)cc...</td>\n",
       "      <td>train</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;deepchem.feat.mol_graphs.ConvMol object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TTZSNFLLYPYKIL-UHFFFAOYSA-N</td>\n",
       "      <td>Cc1cc2cc(Oc3ccnc(Nc4cccc(CS(=O)(=O)NCCN(C)C)c4...</td>\n",
       "      <td>test</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;deepchem.feat.mol_graphs.ConvMol object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UOVCGJXDGOGOCZ-UHFFFAOYSA-N</td>\n",
       "      <td>COc1cc2c(cc1F)C(c1ccccc1Cl)=Nc1c(n[nH]c1C)N2</td>\n",
       "      <td>train</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;deepchem.feat.mol_graphs.ConvMol object at 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUIHSIWYWATEQL-UHFFFAOYSA-N</td>\n",
       "      <td>Cc1ccc(Nc2nccc(N(C)c3ccc4c(C)n(C)nc4c3)n2)cc1S...</td>\n",
       "      <td>test</td>\n",
       "      <td>active</td>\n",
       "      <td>1</td>\n",
       "      <td>[&lt;deepchem.feat.mol_graphs.ConvMol object at 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      inchikey  ...                                         smileToMol\n",
       "0  FNHKPVJBJVTLMP-UHFFFAOYSA-N  ...  [<deepchem.feat.mol_graphs.ConvMol object at 0...\n",
       "1  CUDVHEFYRIWYQD-UHFFFAOYSA-N  ...  [<deepchem.feat.mol_graphs.ConvMol object at 0...\n",
       "2  TTZSNFLLYPYKIL-UHFFFAOYSA-N  ...  [<deepchem.feat.mol_graphs.ConvMol object at 0...\n",
       "3  UOVCGJXDGOGOCZ-UHFFFAOYSA-N  ...  [<deepchem.feat.mol_graphs.ConvMol object at 0...\n",
       "4  CUIHSIWYWATEQL-UHFFFAOYSA-N  ...  [<deepchem.feat.mol_graphs.ConvMol object at 0...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmpd_df[\"smileToMol\"] = None\n",
    "\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "def smileToMol(smile):\n",
    "  # smile = 'CNC(=O)c1cc(Oc2ccc(NC(=O)Nc3ccc(Cl)c(C(F)(F)F)c3)c(F)c2)ccn1'\n",
    "  molecules = []\n",
    "  molecules.append(Chem.MolFromSmiles(smile))\n",
    "  featurizer = dc.feat.graph_features.ConvMolFeaturizer()\n",
    "  mol_object = featurizer.featurize(molecules)\n",
    "  return mol_object\n",
    "\n",
    "for row in cmpd_df.itertuples():\n",
    "  cmpd_df.at[row.Index,'smileToMol'] = smileToMol(row.smiles)\n",
    "cmpd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "go6vzjBNTDEp",
    "outputId": "39509428-46a9-4841-f8b2-1bc0c4b90aeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data_train.shape= (3977, 6) data_test.shape= (1553, 6)\n"
     ]
    }
   ],
   "source": [
    "data_train = cmpd_df[cmpd_df.group == \"train\"]\n",
    "data_test = cmpd_df[cmpd_df.group == \"test\"]\n",
    "data_train.to_csv(\"data_train.csv\", index=False)\n",
    "data_test.to_csv(\"data_test.csv\", index=False)\n",
    "print(\"\\ndata_train.shape=\",data_train.shape, \"data_test.shape=\", data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvd1iOErxU9w",
    "outputId": "3c47458b-2c2c-42f9-f19a-2356448a52bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5530"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3977 + 1553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qIYsyqwSgKVR"
   },
   "outputs": [],
   "source": [
    "### notes: I combined data_train and data_test files into one.\n",
    "### The first 3977 rows idx[0:3976] comes from the data_train and the rest of it belongs to the data_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ln55oLmHU8Gs",
    "outputId": "e56f3b80-ec35-4dd2-d2a8-f8c68f1ad057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-20 10:23:04--  https://github.com/TrinhDinhPhuc/CompoundClassification/blob/main/data.csv.gz?raw=true\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/TrinhDinhPhuc/CompoundClassification/raw/main/data.csv.gz [following]\n",
      "--2021-02-20 10:23:04--  https://github.com/TrinhDinhPhuc/CompoundClassification/raw/main/data.csv.gz\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/TrinhDinhPhuc/CompoundClassification/main/data.csv.gz [following]\n",
      "--2021-02-20 10:23:04--  https://raw.githubusercontent.com/TrinhDinhPhuc/CompoundClassification/main/data.csv.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84823 (83K) [application/octet-stream]\n",
      "Saving to: ‘data.csv.gz?raw=true’\n",
      "\n",
      "data.csv.gz?raw=tru 100%[===================>]  82.83K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2021-02-20 10:23:05 (9.49 MB/s) - ‘data.csv.gz?raw=true’ saved [84823/84823]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Download the final data that was saved\n",
    "!wget https://github.com/TrinhDinhPhuc/CompoundClassification/blob/main/data.csv.gz?raw=true\n",
    "!mv data.csv.gz?raw=true data.csv.gz\n",
    "!mv data.csv.gz /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lniUlqalFdEK"
   },
   "source": [
    "##### The SpecifiedSplitter Class written by Deepchem does not allow me to specify data_train, data_validation, and data_test by using slicing, therefore, I copied SpecifiedSplitter and Splitter classes from Deepchem's source and modified the SpecifiedSplitter class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "53N-rodxrxqh"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import itertools\n",
    "import logging\n",
    "from typing import Any, Dict, List, Iterator, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import deepchem as dc\n",
    "from deepchem.data import Dataset, DiskDataset\n",
    "from deepchem.utils import get_print_threshold\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Splitter(object):\n",
    "  \"\"\"Splitters split up Datasets into pieces for training/validation/testing.\n",
    "  In machine learning applications, it's often necessary to split up a dataset\n",
    "  into training/validation/test sets. Or to k-fold split a dataset (that is,\n",
    "  divide into k equal subsets) for cross-validation. The `Splitter` class is\n",
    "  an abstract superclass for all splitters that captures the common API across\n",
    "  splitter classes.\n",
    "  Note that `Splitter` is an abstract superclass. You won't want to\n",
    "  instantiate this class directly. Rather you will want to use a concrete\n",
    "  subclass for your application.\n",
    "  \"\"\"\n",
    "\n",
    "  def k_fold_split(self,\n",
    "                   dataset: Dataset,\n",
    "                   k: int,\n",
    "                   directories: Optional[List[str]] = None,\n",
    "                   **kwargs) -> List[Tuple[Dataset, Dataset]]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: Dataset\n",
    "      Dataset to do a k-fold split\n",
    "    k: int\n",
    "      Number of folds to split `dataset` into.\n",
    "    directories: List[str], optional (default None)\n",
    "      List of length 2*k filepaths to save the result disk-datasets.\n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[Dataset, Dataset]]\n",
    "      List of length k tuples of (train, cv) where `train` and `cv` are both `Dataset`.\n",
    "    \"\"\"\n",
    "    logger.info(\"Computing K-fold split\")\n",
    "    if directories is None:\n",
    "      directories = [tempfile.mkdtemp() for _ in range(2 * k)]\n",
    "    else:\n",
    "      assert len(directories) == 2 * k\n",
    "    cv_datasets = []\n",
    "    train_ds_base = None\n",
    "    train_datasets = []\n",
    "    # rem_dataset is remaining portion of dataset\n",
    "    if isinstance(dataset, DiskDataset):\n",
    "      rem_dataset = dataset\n",
    "    else:\n",
    "      rem_dataset = DiskDataset.from_numpy(dataset.X, dataset.y, dataset.w,\n",
    "                                           dataset.ids)\n",
    "\n",
    "    for fold in range(k):\n",
    "      # Note starts as 1/k since fold starts at 0. Ends at 1 since fold goes up\n",
    "      # to k-1.\n",
    "      frac_fold = 1. / (k - fold)\n",
    "      train_dir, cv_dir = directories[2 * fold], directories[2 * fold + 1]\n",
    "      fold_inds, rem_inds, _ = self.split(\n",
    "          rem_dataset,\n",
    "          frac_train=frac_fold,\n",
    "          frac_valid=1 - frac_fold,\n",
    "          frac_test=0,\n",
    "          **kwargs)\n",
    "      cv_dataset = rem_dataset.select(fold_inds, select_dir=cv_dir)\n",
    "      cv_datasets.append(cv_dataset)\n",
    "      # FIXME: Incompatible types in assignment (expression has type \"Dataset\", variable has type \"DiskDataset\")\n",
    "      rem_dataset = rem_dataset.select(rem_inds)  # type: ignore\n",
    "\n",
    "      train_ds_to_merge: Iterator[Dataset] = filter(\n",
    "          None, [train_ds_base, rem_dataset])\n",
    "      train_ds_to_merge = filter(lambda x: len(x) > 0, train_ds_to_merge)\n",
    "      train_dataset = DiskDataset.merge(train_ds_to_merge, merge_dir=train_dir)\n",
    "      train_datasets.append(train_dataset)\n",
    "\n",
    "      update_train_base_merge: Iterator[Dataset] = filter(\n",
    "          None, [train_ds_base, cv_dataset])\n",
    "      train_ds_base = DiskDataset.merge(update_train_base_merge)\n",
    "    return list(zip(train_datasets, cv_datasets))\n",
    "\n",
    "  def train_valid_test_split(self,\n",
    "                             dataset: Dataset,\n",
    "                             train_dir: Optional[str] = None,\n",
    "                             valid_dir: Optional[str] = None,\n",
    "                             test_dir: Optional[str] = None,\n",
    "                             frac_train: float = 0.8,\n",
    "                             frac_valid: float = 0.1,\n",
    "                             frac_test: float = 0.1,\n",
    "                             seed: Optional[int] = None,\n",
    "                             log_every_n: int = 1000,\n",
    "                             **kwargs) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    \"\"\" Splits self into train/validation/test sets.\n",
    "    Returns Dataset objects for train, valid, test.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: Dataset\n",
    "      Dataset to be split.\n",
    "    train_dir: str, optional (default None)\n",
    "      If specified, the directory in which the generated\n",
    "      training dataset should be stored. This is only\n",
    "      considered if `isinstance(dataset, dc.data.DiskDataset)`\n",
    "    valid_dir: str, optional (default None)\n",
    "      If specified, the directory in which the generated\n",
    "      valid dataset should be stored. This is only\n",
    "      considered if `isinstance(dataset, dc.data.DiskDataset)`\n",
    "      is True.\n",
    "    test_dir: str, optional (default None)\n",
    "      If specified, the directory in which the generated\n",
    "      test dataset should be stored. This is only\n",
    "      considered if `isinstance(dataset, dc.data.DiskDataset)`\n",
    "      is True.\n",
    "    frac_train: float, optional (default 0.8)\n",
    "      The fraction of data to be used for the training split.\n",
    "    frac_valid: float, optional (default 0.1)\n",
    "      The fraction of data to be used for the validation split.\n",
    "    frac_test: float, optional (default 0.1)\n",
    "      The fraction of data to be used for the test split.\n",
    "    seed: int, optional (default None)\n",
    "      Random seed to use.\n",
    "    log_every_n: int, optional (default 1000)\n",
    "      Controls the logger by dictating how often logger outputs\n",
    "      will be produced.\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dataset, Optional[Dataset], Dataset]\n",
    "      A tuple of train, valid and test datasets as dc.data.Dataset objects.\n",
    "    \"\"\"\n",
    "    logger.info(\"Computing train/valid/test indices\")\n",
    "    train_inds, valid_inds, test_inds = self.split(\n",
    "        dataset,\n",
    "        seed=seed,\n",
    "        log_every_n=log_every_n)\n",
    "    if train_dir is None:\n",
    "      train_dir = tempfile.mkdtemp()\n",
    "    if valid_dir is None:\n",
    "      valid_dir = tempfile.mkdtemp()\n",
    "    if test_dir is None:\n",
    "      test_dir = tempfile.mkdtemp()\n",
    "    train_dataset = dataset.select(train_inds, train_dir)\n",
    "    valid_dataset = dataset.select(valid_inds, valid_dir)\n",
    "    test_dataset = dataset.select(test_inds, test_dir)\n",
    "    if isinstance(train_dataset, DiskDataset):\n",
    "      train_dataset.memory_cache_size = 40 * (1 << 20)  # 40 MB\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "  def train_test_split(self,\n",
    "                       dataset: Dataset,\n",
    "                       train_dir: Optional[str] = None,\n",
    "                       test_dir: Optional[str] = None,\n",
    "                       frac_train: float = 0.8,\n",
    "                       seed: Optional[int] = None,\n",
    "                       **kwargs) -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"Splits self into train/test sets.\n",
    "    Returns Dataset objects for train/test.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: data like object\n",
    "      Dataset to be split.\n",
    "    train_dir: str, optional (default None)\n",
    "      If specified, the directory in which the generated\n",
    "      training dataset should be stored. This is only\n",
    "      considered if `isinstance(dataset, dc.data.DiskDataset)`\n",
    "      is True.\n",
    "    test_dir: str, optional (default None)\n",
    "      If specified, the directory in which the generated\n",
    "      test dataset should be stored. This is only\n",
    "      considered if `isinstance(dataset, dc.data.DiskDataset)`\n",
    "      is True.\n",
    "    frac_train: float, optional (default 0.8)\n",
    "      The fraction of data to be used for the training split.\n",
    "    seed: int, optional (default None)\n",
    "      Random seed to use.\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dataset, Dataset]\n",
    "      A tuple of train and test datasets as dc.data.Dataset objects.\n",
    "    \"\"\"\n",
    "    valid_dir = tempfile.mkdtemp()\n",
    "    train_dataset, _, test_dataset = self.train_valid_test_split(\n",
    "        dataset,\n",
    "        train_dir,\n",
    "        valid_dir,\n",
    "        test_dir,\n",
    "        frac_train=frac_train,\n",
    "        frac_test=1 - frac_train,\n",
    "        frac_valid=0.,\n",
    "        seed=seed,\n",
    "        **kwargs)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "  def split(self,\n",
    "            dataset: Dataset,\n",
    "            frac_train: float = 0.8,\n",
    "            frac_valid: float = 0.1,\n",
    "            frac_test: float = 0.1,\n",
    "            seed: Optional[int] = None,\n",
    "            log_every_n: Optional[int] = None) -> Tuple:\n",
    "    \"\"\"Return indices for specified split\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: dc.data.Dataset\n",
    "      Dataset to be split.\n",
    "    seed: int, optional (default None)\n",
    "      Random seed to use.\n",
    "    frac_train: float, optional (default 0.8)\n",
    "      The fraction of data to be used for the training split.\n",
    "    frac_valid: float, optional (default 0.1)\n",
    "      The fraction of data to be used for the validation split.\n",
    "    frac_test: float, optional (default 0.1)\n",
    "      The fraction of data to be used for the test split.\n",
    "    log_every_n: int, optional (default None)\n",
    "      Controls the logger by dictating how often logger outputs\n",
    "      will be produced.\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "      A tuple `(train_inds, valid_inds, test_inds)` of the indices (integers) for\n",
    "      the various splits.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def __str__(self) -> str:\n",
    "    \"\"\"Convert self to str representation.\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "      The string represents the class.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import deepchem as dc\n",
    "    >>> str(dc.splits.RandomSplitter())\n",
    "    'RandomSplitter'\n",
    "    \"\"\"\n",
    "    args_spec = inspect.getfullargspec(self.__init__)  # type: ignore\n",
    "    args_names = [arg for arg in args_spec.args if arg != 'self']\n",
    "    args_num = len(args_names)\n",
    "    args_default_values = [None for _ in range(args_num)]\n",
    "    if args_spec.defaults is not None:\n",
    "      defaults = list(args_spec.defaults)\n",
    "      args_default_values[-len(defaults):] = defaults\n",
    "\n",
    "    override_args_info = ''\n",
    "    for arg_name, default in zip(args_names, args_default_values):\n",
    "      if arg_name in self.__dict__:\n",
    "        arg_value = self.__dict__[arg_name]\n",
    "        # validation\n",
    "        # skip list\n",
    "        if isinstance(arg_value, list):\n",
    "          continue\n",
    "        if isinstance(arg_value, str):\n",
    "          # skip path string\n",
    "          if \"\\\\/.\" in arg_value or \"/\" in arg_value or '.' in arg_value:\n",
    "            continue\n",
    "        # main logic\n",
    "        if default != arg_value:\n",
    "          override_args_info += '_' + arg_name + '_' + str(arg_value)\n",
    "    return self.__class__.__name__ + override_args_info\n",
    "\n",
    "  def __repr__(self) -> str:\n",
    "    \"\"\"Convert self to repr representation.\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "      The string represents the class.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import deepchem as dc\n",
    "    >>> dc.splits.RandomSplitter()\n",
    "    RandomSplitter[]\n",
    "    \"\"\"\n",
    "    args_spec = inspect.getfullargspec(self.__init__)  # type: ignore\n",
    "    args_names = [arg for arg in args_spec.args if arg != 'self']\n",
    "    args_info = ''\n",
    "    for arg_name in args_names:\n",
    "      value = self.__dict__[arg_name]\n",
    "      # for str\n",
    "      if isinstance(value, str):\n",
    "        value = \"'\" + value + \"'\"\n",
    "      # for list\n",
    "      if isinstance(value, list):\n",
    "        threshold = get_print_threshold()\n",
    "        value = np.array2string(np.array(value), threshold=threshold)\n",
    "      args_info += arg_name + '=' + str(value) + ', '\n",
    "    return self.__class__.__name__ + '[' + args_info[:-2] + ']'\n",
    "\n",
    "\n",
    "\n",
    "### *******************************************************\n",
    "### -------------------------------------------------------\n",
    "### -------------------------------------------------------\n",
    "### -------------------------------------------------------\n",
    "### I did modify the following class to split the dataset by index\n",
    "\n",
    "class SpecifiedSplitter(Splitter):\n",
    "  def __init__(self,\n",
    "              #  train_indices = None,\n",
    "               valid_indices = list(range(3001, 3977)), # 3001 -> 3976\n",
    "               test_indices  = list(range(3977, 5530))): # 3977:\n",
    "\n",
    "    self.valid_indices = valid_indices\n",
    "    self.test_indices = test_indices\n",
    "  def split(self,\n",
    "            dataset: Dataset,\n",
    "            seed: Optional[int] = None,\n",
    "            log_every_n: Optional[int] = None\n",
    "           ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    train_indices = list(range(0, 3001)) # 0 -> 3000\n",
    "    return (np.array(train_indices), np.array(self.valid_indices),\n",
    "            np.array(self.test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fzDjaD2tTdZc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import deepchem as dc\n",
    "from deepchem.molnet.load_function.molnet_loader import TransformerGenerator, _MolnetLoader\n",
    "from deepchem.data import Dataset\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "# from deepchem.splits.splitters import SpecifiedSplitter\n",
    "\n",
    "TASKS = [\n",
    "    'active', 'inactive'\n",
    "]\n",
    "\n",
    "class _Loader(_MolnetLoader):\n",
    "  def create_dataset(self) -> Dataset:\n",
    "    dataset_file = os.path.join(self.data_dir, \"data.csv.gz\")\n",
    "    if not os.path.exists(dataset_file):\n",
    "      print(\"The file does not exist !!!\")\n",
    "    loader = dc.data.CSVLoader(\n",
    "        tasks=self.tasks, feature_field=\"smiles\", featurizer=self.featurizer)\n",
    "    return loader.create_dataset(dataset_file, shard_size=8192)\n",
    "\n",
    "def load_dataset(\n",
    "    featurizer: Union[dc.feat.Featurizer, str] = 'ECFP',\n",
    "    splitter = SpecifiedSplitter(),\n",
    "    transformers: List[Union[TransformerGenerator, str]] = ['balancing'],\n",
    "    reload: bool = True,\n",
    "    data_dir: Optional[str] = None,\n",
    "    save_dir: Optional[str] = None,\n",
    "    **kwargs\n",
    "\n",
    ") -> Tuple[List[str], Tuple[Dataset, ...], List[dc.trans.Transformer]]:\n",
    "\n",
    "  loader = _Loader(featurizer, splitter, transformers, TASKS,\n",
    "                        data_dir, save_dir, **kwargs)\n",
    "  return loader.load_dataset('data_train', reload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "N7ubOYOb76km"
   },
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "tasks, datasets, transformers = load_dataset(featurizer='GraphConv')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "vv0xhw9s8Cwl",
    "outputId": "fde41432-f66f-4b4e-9d2e-96c67aa7e045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3001 976 1553\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' \\n- 3001 rows for training\\n- 976 rows for validation\\n- 1553 rows for testing\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dataset.X.shape[0], valid_dataset.X.shape[0], test_dataset.X.shape[0])\n",
    "''' \n",
    "- 3001 rows for training\n",
    "- 976 rows for validation\n",
    "- 1553 rows for testing\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SR34q0wgyJsR",
    "outputId": "b39e1bb2-d375-4051-ed43-d6626ce605b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5530"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.X.shape[0] + valid_dataset.X.shape[0] + test_dataset.X.shape[0]\n",
    "## it must return 5530 to be correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2jotWCfKdVV",
    "outputId": "b7866c2c-d94e-4980-aeb5-8525e68bce66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4023862838745117"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I use a simple Graph Convolutional Model provided by deepchem to do the classification\n",
    "## https://deepchem.readthedocs.io/en/latest/api_reference/models.html#graphconvmodel\n",
    "model = dc.models.GraphConvModel(2, # n_tasks=2  \n",
    "                                 mode='classification', \n",
    "                                 dropout = 0.25, ## we are facing overfiting, thus I set dropout = 0.25 to tackle the problem\n",
    "                                 dense_layer_size =128, # dense_layer_size =128 as default\n",
    "                                 tensorboard=True,\n",
    "                                 model_dir = \"/tmp/tensorboard\")\n",
    "model.fit(train_dataset, nb_epoch=200)\n",
    "## Note: the training process will take around 1 to 2 mins only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCQhkKG2KkiB",
    "outputId": "51899fe7-caec-46f9-ece5-aff1406b0c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'roc_auc_score': 0.9875792243743441}\n",
      "Test set score: {'roc_auc_score': 0.925342893340433}\n"
     ]
    }
   ],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('Training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('Test set score:', model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MugWxINuGQiS"
   },
   "source": [
    "## The results will be something like these:\n",
    "#### Training set score: {'roc_auc_score': 0.9899005978872861}\n",
    "#### Test set score: {'roc_auc_score': 0.9482054330462883}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WU1OVu_B8Hc7"
   },
   "source": [
    "### Please ignore the following cells, They are just my customized model that I want to play around with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "zy9_jr1CKmax"
   },
   "outputs": [],
   "source": [
    "from deepchem.models.layers import GraphConv, GraphPool, GraphGather\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "batch_size = 100\n",
    "n_tasks = 2\n",
    "\n",
    "class MyGraphConvModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(MyGraphConvModel, self).__init__()\n",
    "    self.gc1 = GraphConv(128, activation_fn=tf.nn.tanh)\n",
    "    self.batch_norm1 = layers.BatchNormalization()\n",
    "    self.gp1 = GraphPool()\n",
    "\n",
    "    self.gc2 = GraphConv(128, activation_fn=tf.nn.tanh)\n",
    "    self.batch_norm2 = layers.BatchNormalization()\n",
    "    self.gp2 = GraphPool()\n",
    "\n",
    "    self.dense1 = layers.Dense(256, activation=tf.nn.tanh)\n",
    "    self.batch_norm3 = layers.BatchNormalization()\n",
    "    self.readout = GraphGather(batch_size=batch_size, activation_fn=tf.nn.tanh)\n",
    "\n",
    "    self.dense2 = layers.Dense(n_tasks*2)\n",
    "    self.logits = layers.Reshape((n_tasks, 2))\n",
    "    self.softmax = layers.Softmax()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    gc1_output = self.gc1(inputs)\n",
    "    batch_norm1_output = self.batch_norm1(gc1_output)\n",
    "    gp1_output = self.gp1([batch_norm1_output] + inputs[1:])\n",
    "\n",
    "    gc2_output = self.gc2([gp1_output] + inputs[1:])\n",
    "    batch_norm2_output = self.batch_norm1(gc2_output)\n",
    "    gp2_output = self.gp2([batch_norm2_output] + inputs[1:])\n",
    "\n",
    "    dense1_output = self.dense1(gp2_output)\n",
    "    batch_norm3_output = self.batch_norm3(dense1_output)\n",
    "    readout_output = self.readout([batch_norm3_output] + inputs[1:])\n",
    "\n",
    "    logits_output = self.logits(self.dense2(readout_output))\n",
    "    return self.softmax(logits_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "J8x1rHkoKnzE"
   },
   "outputs": [],
   "source": [
    "model = dc.models.KerasModel(\n",
    "    MyGraphConvModel(), \n",
    "    loss=dc.models.losses.CategoricalCrossEntropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-Q_9kcl5KoHu"
   },
   "outputs": [],
   "source": [
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.feat.mol_graphs import ConvMol\n",
    "import numpy as np\n",
    "\n",
    "def data_generator(dataset, epochs=1):\n",
    "  for ind, (X_b, y_b, w_b, ids_b) in enumerate(dataset.iterbatches(batch_size, epochs,\n",
    "                                                                   deterministic=False, pad_batches=True)):\n",
    "    multiConvMol = ConvMol.agglomerate_mols(X_b)\n",
    "    inputs = [multiConvMol.get_atom_features(), multiConvMol.deg_slice, np.array(multiConvMol.membership)]\n",
    "    for i in range(1, len(multiConvMol.get_deg_adjacency_lists())):\n",
    "      inputs.append(multiConvMol.get_deg_adjacency_lists()[i])\n",
    "    labels = [to_one_hot(y_b.flatten(), 2).reshape(-1, n_tasks, 2)]\n",
    "    weights = [w_b]\n",
    "    yield (inputs, labels, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmGRxe-YKoJ_",
    "outputId": "3e8157d5-49dc-4d98-8c52-410d7c231f30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11460309028625489"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(data_generator(train_dataset, epochs=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vap-BJhbK13k",
    "outputId": "6bdfa7ec-8f8d-4ffc-c318-e09af951062b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: {'roc_auc_score': 0.7089584658068913}\n",
      "Test set score: {'roc_auc_score': 0.6752864822989295}\n"
     ]
    }
   ],
   "source": [
    "print('Training set score:', model.evaluate_generator(data_generator(train_dataset), [metric], transformers))\n",
    "print('Test set score:', model.evaluate_generator(data_generator(test_dataset), [metric], transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0x1EpOjzrva"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZQO03BDvny44Q9XHmWBhq",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Compound Classification using Graph Convolutional Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
